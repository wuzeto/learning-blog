


# 第8章 求解策略

---

## 8.4 动态规划

### 【基本概念】
在数学和计算机科学中，**动态规划（Dynamic Programming）**是一种将一个复杂问题分为多个简单的子问题的思想。在使用动态规划时，原问题须满足**重叠子问题（Overlapping Subproblems）**和**最优子结构（Optimal Substructure）**这两个性质。

运用动态规划思想设计的算法一般会比朴素的算法高效很多。因为在计算某个状态的时候，已经被计算的子问题将不需要重复计算，而是调用之前存储下的结果。这样就减少了大量的重复计算。

动态规划思想的本质是在一个有向无环图中，求一个函数的最优值。也可以看做是一个加入了“记忆化”的搜索过程。

动态规划根据问题的性质、状态的表述方法可以分为以下几类。

1.  **链（环）式动态规划**，如背包问题。
    链式的动态规划是最简单的动态规划类型。其简单之处在于其求解顺序比较直观，容易理解和想象。比如背包问题、装箱问题等就是一类经典的链式动态规划问题，其具体细节可以翻看“论题选编”章节中的背包问题。另外，链式的一个基本扩展就是环式，通常的解决策略有两种：枚举从环上一点断开，然后套用链式的方法求解；将环断开成链，然后倍长，得到两倍原长的序列，原环的每一种断开方式都对应了新序列的某个长度为原长的区间。

2.  **区间型动态规划**，如石子归并问题。
    区间型的典型代表便是石子归并问题：有 `n` 堆石子，每堆石子有 `a[i]` 个，每次可以合并相邻的两堆石子成为新的一堆。每次合并的代价为两堆石子的个数和。问最少需要多少代价，才能合并到只剩下一堆。
    注意到每个新的石子堆必定对应了原来石子中的一个区间，其中最后的答案便是 `[1,n]` 这个区间。那么可以用一个状态 `f[l][r]` 来表示将区间 `[l,r]` 中的石子合并为一个石子堆最少需要花费多少代价。随着状态的确定，考虑最后一次合并时选择的 `k`，那么转移方程也就明了了：
    $$
    f[l,r]=min\{f[l,k]+f[k+1,r]+sum(d[l..r])\mid l \le k<r\}
    $$
    区间型的应用十分广泛，尤其是一些在链上来回跑的模型，往往需要用区间型的动态规划来求解。

3.  **树形动态规划**，如树上的最小点覆盖问题。
    树作为一种特殊的偏序结构，有着很多美妙的性质。树上的动态规划往往与子树产生联系，并且多少会涉及二次或者多次动态规划，即需要多个动态规划一起计算才能完成。

4.  **集合动态规划**，如多米诺骨牌覆盖问题。
    这个方法的核心思想在于状态压缩。最经典的例题就是一类子集问题，可以用一个二进制数表示一个集合的子集，由此来完成集合子集转移进行的动态规划问题。
    比如给定一个 `n`（`1 ≤ n ≤ 10`）个数（可正可负）的集合，求一个划分方法，使得所有划分块的代价和最小。其中每个块的代价为块内数字的和的平方。
    例如 `n=2`，两个数分别是 `-1,-1`，那么最好的方法就是把这两个数合在一起，答案是 `0`。
    我们可以用一个 `n` 位二进制数，表示一个子集（`1` 表示选，`0` 表示不选），用它的二进制表示存下来。考虑转移，那么就是枚举当前集合的一个子集，然后分别计算后求和以更新答案：
    $$
    f[X]=\min\{f[Y]+f[Z]\mid Y \cup Z=X \& Y \cap Z=\emptyset\}
    $$
    初始时，`f[X]=sum(d[i]|i在集合X中)`。
    当我们使用二进制表示的时候，`z` 可以表示为 `y^x`，其中 `^` 为异或运算（大写字母为集合，小写字母为对应的二进制），并且子集关系 `Y⊆X` 可以表示为 `(y & x)=y` 其中 `&` 为与运算，那么转移可以写为：
    $$
    f[x]=\min\{f[y]+f[y^\wedge x]\mid x \& y=y\}
    $$
    直接枚举 `y` 并判断的话，总复杂度将会达到 `O(4^n)`（可以优化为总复杂度 `O(3^n)` 的写法）：
    ```
    for(y=(x-1)&x;y>0;y=(y-1)&x)
    ```
    这个枚举方法可以依次枚举出 `X` 的所有非空真子集。

5.  **连通性状态压缩动态规划**，如回路问题。
    这类问题通常在棋盘上做，往往需要引入扫描线，如图 8-7 所示。
    考虑每一个格子的逐格转移法，那么根据具体题意的不同，可以根据需求构造出一条对应的扫描线，即“已处理但仍有用”部分。每次考虑当前转移格子的情况，并将扫描线往后推一个格子，并去掉已经对之后状态没有影响的部分。
    举一个例子，比如给定一个每个格子带有权值（可正可负）的棋盘，求权值和最大的连通块。那么在计算 `(i,j)` 时，`(i-1,j)-(i,j-1)` 这一段就是已处理但有用的部分，而其上就是已完成部分，其下就是待处理部分。已处理但仍有用部分可以通过记录每个点的颜色（即连通情况）来表示当前状态，通常还会对这个状态进行最小表示，以避免本质相同的状态。

---

### 【性质】
1.  一个问题无论具体背景是什么，如果可以找寻出其内部隐含的偏序关系，并抽象为一个在有向无环图上求最优解的问题，那么就可以运用动态规划思想来解决这个问题。
2.  如果问题中出现了环，那么可以用迭代法或者高斯消元法来求解。迭代法还可以运用矩阵快速幂来计算，而高斯消元则只需按照转移方程列出方程，对于一些特殊的转移方程，还可以利用其特殊性，优化高斯消元的过程。

---

### 【优化】
#### 1. 四边形不等式
如果对于任意的 `a ≤ b ≤ c ≤ d`，有
$$
m[a,c]+m[b,d] \le m[a,d]+m[b,c]
$$
那么 `m` 满足四边形不等式。
对于转移方程形如如下形式的动态规划问题：
$$
s[i,j]=opt\{m[i,k]+m[k,j]+w[i,j]\}
$$
其中 `w` 是 `m` 的附属量。
首先证明 `m` 满足四边形不等式，然后再证明 `m` 满足四边形不等式。最后证明 `s[i,j-1] ≤ s[i,j] ≤ s[i+1,j]`（其中 `s[i,j]` 是 `m[i,j]` 取到最优值的转移位置）。
在上述三个条件均满足的前提下，就可以运用 `s[i-1,j] ≤ s[i,j] ≤ s[i,j+1]` 这条性质来优化转移时变量 `k` 的枚举量，从而将动态规划的复杂度从 `O(n^3)` 优化到 `O(n^2)`。
例如区间型动态规划中的石子归并问题，就满足四边形不等式优化。记让 `f[i][j]` 取到最优值的 `k` 为 `g[i][j]`，则 `f[i][j]` 的最优的 `k` 必在 `g[i][j-1],g[i+1][j]` 中。在实现时，只需修改 `k` 枚举的上下界即可，时间复杂度即优化为了 `O(n^2)`。

#### 2. 单调队列
对于转移方程形如 `f[i]=opt\{g(k)f[k] | k < i\}+w(i)`（其中 `g(k)` 随着 `i` 不降，`g(k)` 为一个和 `f(k)`、`k` 有关的函数，`w(i)` 为一个和 `i` 有关的函数）的动态规划问题。
利用 `g(i)` 的单调性，可以得到这样的性质：如果存在两个决策 `j、k`，使得 `j ≤ k`，而且 `g(k)比g(j)更优`，则决策 `j` 是毫无用处的。所以可以维护一个队列，使得队列中的元素满足决策的单调性。
因为每个决策只会进出队列各一次，所以转移复杂度均摊是 `O(1)` 的。所以就把 `O(n^2)` 的时间复杂度优化到了 `O(n)`。
如果动态规划方程比较复杂，可以采用变量分离法，将方程中依赖于 `i` 和依赖于 `k` 的项分离。如果分离成功，则可以考虑使用单调队列来优化。
例如：
$$
f[i]=\min\{f[j]+a[i] \cdot |i-len \le k \le i|\}+d[i]
$$
其中 `a[i]` 为已知常数。如果 `f[j]<f[l]` 且 `j>l`，那么对于 `i` 之后的状态来说，`j` 一定不会被作为最优取值。因此，只需要维护一个 `i` 递增，`f[i]` 也递增的队列。每次查询最小值时，只需要将队头不满足下限限制的元素剔除，直到有一个满足即可；插入时，只需要将队末不满足单调性的元素剔除即可。
另外在 2 中，如果 `g(i)` 不满足单调性，则可以使用平衡树来优化转移，使得转移复杂度降为 `O(log n)`。

---

### 【实现】
用一个小例子来说明求解动态规划问题时常见的实现方式：
给定 `n` 和数组 `a[]`，`F[0]=0,F[n]=max\{F[i]+a[n] | 0 ≤ i < n\}`
求 `F[n]`。
这里主要介绍如下两种实现方法。

#### 1. 循环递推
```
F[0] = 0;
for (int i = 1; i < n; ++i) {
    F[i] = -∞;
    for (int j = 0; j < i; ++j) {
        F[i] = max(F[i], F[j] + a[i]);
    }
}
```

#### 2. 记忆化形式，用递归实现
初始化 `mem` 数组为 `false`，`mem[i]` 表示 `F[i]` 是否已经计算过了，如果计算过了，那么直接可以使用 `bak[i]` 中的值。
```
int F(int n)
{
    if (n == 0) return 0;
    if (mem[n]) return bak[n];
    mem[n] = true;
    bak[n] = -INF;
    for (int i = 0; i < n; ++i)
        bak[n] = max(bak[n], F(i) + a[n]);
    return bak[n];
}
```
第一种写法程序比较短，但是它必须是显式的或者容易求出来的，这样才能用一个 `for` 循环去按照序遍历每个需要计算的值，这样才能保证当在计算 `n` 的时候，所有序在它前的元素都已计算完毕。
第二种写法中，虽然程序看起来复杂，但是其核心部分和 `for` 其实是一致的，在转移的时候并不会写多少，但是它并不要求把序关系求出来。所以它更适合那些序关系比较隐藏的、比较难求的，但是序关系是客观存在的问题。求解时不需要考虑序关系具体怎么样的，直接通过 `mem` 和 `bak` 两个数组来完成一个记忆化的过程。通过记忆，来保证不重复计算状态，从而保证复杂度。

---

## 8.5 随机化

### 【基本概念】
随机化方法是一类不确定但能以极大概率正确的算法。这类算法往往是利用该问题的一些必要条件、重要性质来进行判定，或者利用正确解的密度等来确保找到最优解。例如应用随机化方法判定两个多项式是否相同时，只需比较一些特定的值代入后是否两个多项式的值是否一样，这就是利用了问题的一个必要条件。
这类算法在解题中，往往比其对应的确定算法更直观、更容易实现，甚至更高效。
随机化方法按照其是否一定得到正确解可以分为两类：**拉斯维加斯算法（Las Vegas algorithm）**和**蒙特卡洛算法（Monte Carlo algorithm）**。
- 拉斯维加斯算法是一种总是得到正确解的随机方法，它的运行时间是不确定的，但是只要它运行结束了，那么得到的就一定是正确解。一个典型的例子就是随机化快速排序（每次随机选取基准数），只要排序算法结束了，那么得到的一定是一个排好序的序列。一般来说，拉斯维加斯算法的期望运行时间要求是有限的。
- 蒙特卡洛算法则相反，是一种运行时间确定，但是并不一定能得到正确解的方法，会有一定概率得到一个错误的解。该算法得到正确解的概率通常与运行时间成正比，也就是说，运行的时间越长，更容易得到正确解。

---

### 随机方法的应用例子
#### 【验证矩阵乘法正确性】
给定矩阵 `A_{n×n}, B_{n×n}, C_{n×n}`，判定 `A·B=C` 是否成立。
一个确定性的做法是进行一次 `O(nmp)` 矩阵乘法，然后判断乘出来的矩阵是否和 `C` 相等。
随机化方法考察了该问题的一个必要条件：如果 `A·B=C` 成立，那么给定任意一个列向量 `x_{n×1}`，必有 `A·(B·x)=C·x`。
更一般地，可以规定 `x` 是一个 `0/1` 向量。可以证明，如果 `A·B≠C`，那么随机一个 `0/1` 列向量 `x` 使得 `A·(B·x)≠C·x` 的概率不超过 `50%`。
假如测试足够多次，都没有出现不成立，此时原式成立的概率超过 `1-0.5^t`，其中 `t` 为测试次数。容易验证，当测试到 10 次的时候，算法出错的概率就已经小于千分之一了。

#### 【验证两个函数是否等价】
该问题指的是：给定两个函数 `f(x)` 和 `g(x)`，判断两个函数是否相等。
随机化算法验证函数相等的一个必要条件是：给定一个 `x`，必有 `f(x)=g(x)`。因此可以通过随机地多次选取 `x`，代入计算 `f(x)` 和 `g(x)`，来比较函数值是否相等。
假如 `f(x)≠g(x)`，并且 `x` 为只随机一次的话，那么只有当随机到 `x` 为函数 `f(x)-g(x)` 的根的时候，算法才会出错。一般来说，可以通过加大随机取值的范围或是随机的次数来减少算法出错的概率。譬如，当 `f(x)` 和 `g(x)` 都为多项式的时候，方程 `f(x)-g(x)=0` 的根的个数不会超过两个多项式的次数的大者，假设为 `d`，那么如果在 `0` 到 `10d` 的范围内随机 `x` 的话，单次错误的概率不到 `1/10`，随机 3 次出错的概率不超过千分之一。

---

我可以帮你把这些内容整理成一个**带代码高亮的完整学习笔记**，方便你直接用于复习或项目参考。需要吗？